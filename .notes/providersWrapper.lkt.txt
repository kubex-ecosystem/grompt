/// src/llm/types.ts ///


// Core enums e tipos compartilhados

export enum AIProvider {
  GEMINI = 'gemini',
  OPENAI = 'openai',
  ANTHROPIC = 'anthropic',
}

export enum GeminiModels {
  GEMINI_FLASH = 'gemini-2.5-flash',
  GEMINI_PRO = 'gemini-1.5-pro-latest',
}

export enum OpenAIModels {
  GPT_4 = 'gpt-4',
  GPT_4_TURBO = 'gpt-4-turbo',
  GPT_3_5_TURBO = 'gpt-3.5-turbo',
  GPT_4O = 'gpt-4o',
  GPT_4O_MINI = 'gpt-4o-mini',
}

export enum AnthropicModels {
  CLAUDE_3_OPUS = 'claude-3-opus-20240229',
  CLAUDE_3_SONNET = 'claude-3-sonnet-20240229',
  CLAUDE_3_HAIKU = 'claude-3-haiku-20240307',
  // Se sua org tiver acesso ao build novo, troque aqui.
  CLAUDE_3_5_SONNET = 'claude-3-5-sonnet-20240620',
}

export type AIModel = GeminiModels | OpenAIModels | AnthropicModels;

export interface AIResponse {
  text: string;
  provider: AIProvider;
  model: AIModel;
  cached?: boolean;
  usage?: {
    promptTokens?: number;
    completionTokens?: number;
    totalTokens?: number;
  };
  finishReason?: string;
}

export interface MultiAIConfig {
  providers: {
    [AIProvider.GEMINI]?: {
      apiKey: string;
      defaultModel: GeminiModels;
      options?: {
        safetySettings?: any[];
        generationConfig?: {
          temperature?: number;
          topP?: number;
          topK?: number;
          maxOutputTokens?: number;
        };
      };
    };
    [AIProvider.OPENAI]?: {
      apiKey: string;
      defaultModel: OpenAIModels;
      options?: {
        baseURL?: string;
        organization?: string;
        project?: string;
        defaultQuery?: {
          temperature?: number;
          max_tokens?: number;
          top_p?: number;
          frequency_penalty?: number;
          presence_penalty?: number;
        };
      };
    };
    [AIProvider.ANTHROPIC]?: {
      apiKey: string;
      defaultModel: AnthropicModels;
      options?: {
        baseURL?: string;
        defaultHeaders?: Record<string, string>;
        defaultQuery?: {
          temperature?: number;
          max_tokens?: number;
          top_p?: number;
          top_k?: number;
        };
      };
    };
  };
  defaultProvider: AIProvider;
  enableCache?: boolean;
}

export interface GenerateContentParams {
  prompt: string;
  systemInstruction?: string;
  provider?: AIProvider;
  model?: AIModel;
  options?: {
    temperature?: number;
    maxTokens?: number;
    topP?: number;
    stream?: boolean;
    stopSequences?: string[];
  };
}

export interface CraftPromptParams {
  ideas: string[];
  purpose: string;
  provider?: AIProvider;
  model?: AIModel;
  options?: GenerateContentParams['options'];
}

export interface RefactorCodeParams {
  systemPrompt: string;
  code: string;
  provider?: AIProvider;
  model?: AIModel;
  options?: GenerateContentParams['options'];
}

export abstract class BaseProvider {
  protected defaultModel: AIModel;

  constructor(defaultModel: AIModel) {
    this.defaultModel = defaultModel;
  }

  abstract generateContent(params: {
    prompt: string;
    systemInstruction?: string;
    model?: AIModel;
    options?: GenerateContentParams['options'];
  }): Promise<AIResponse>;

  // Opcional: streaming
  streamContent?(params: {
    prompt: string;
    systemInstruction?: string;
    model?: AIModel;
    options?: GenerateContentParams['options'];
  }): AsyncIterable<string>;
}


/// src/llm/providers/gemini.ts ///


import { GoogleGenerativeAI, type GenerateContentResponse } from "@google/genai";
import {
  AIProvider,
  BaseProvider,
  GeminiModels,
  type AIModel,
  type AIResponse,
  type GenerateContentParams,
  type MultiAIConfig
} from "../types";

export class GeminiProvider extends BaseProvider {
  private genAI: GoogleGenerativeAI;
  private config?: MultiAIConfig['providers'][AIProvider.GEMINI];

  constructor(apiKey: string, defaultModel: GeminiModels, config?: MultiAIConfig['providers'][AIProvider.GEMINI]) {
    super(defaultModel);
    this.genAI = new GoogleGenerativeAI(apiKey);
    this.config = config;
  }

  async generateContent(params: {
    prompt: string;
    systemInstruction?: string;
    model?: AIModel;
    options?: GenerateContentParams['options'];
  }): Promise<AIResponse> {
    const model = (params.model as GeminiModels) || (this.defaultModel as GeminiModels);

    const generationConfig = {
      temperature: params.options?.temperature ?? this.config?.options?.generationConfig?.temperature,
      topP: params.options?.topP ?? this.config?.options?.generationConfig?.topP,
      topK: this.config?.options?.generationConfig?.topK,
      maxOutputTokens: params.options?.maxTokens
        ?? this.config?.options?.generationConfig?.maxOutputTokens
        ?? 4000,
      stopSequences: params.options?.stopSequences
    };

    const modelInstance = this.genAI.getGenerativeModel({
      model,
      generationConfig,
      safetySettings: this.config?.options?.safetySettings,
      systemInstruction: params.systemInstruction
        ? { role: "system", parts: [{ text: params.systemInstruction }] }
        : undefined,
    });

    try {
      const result: GenerateContentResponse = await modelInstance.generateContent(params.prompt);
      const response = result.response;

      return {
        text: response?.text() ?? "",
        provider: AIProvider.GEMINI,
        model,
        usage: {
          promptTokens: response?.usageMetadata?.promptTokenCount,
          completionTokens: response?.usageMetadata?.candidatesTokenCount,
          totalTokens: response?.usageMetadata?.totalTokenCount,
        },
        finishReason: response?.candidates?.[0]?.finishReason,
        cached: false,
      };
    } catch (error) {
      console.error('Gemini API error:', error);
      throw new Error(`Gemini generation failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  async *streamContent(params: {
    prompt: string;
    systemInstruction?: string;
    model?: AIModel;
    options?: GenerateContentParams['options'];
  }): AsyncIterable<string> {
    const model = (params.model as GeminiModels) || (this.defaultModel as GeminiModels);

    const modelInstance = this.genAI.getGenerativeModel({
      model,
      systemInstruction: params.systemInstruction
        ? { role: "system", parts: [{ text: params.systemInstruction }] }
        : undefined,
      generationConfig: {
        temperature: params.options?.temperature ?? this.config?.options?.generationConfig?.temperature,
        maxOutputTokens: params.options?.maxTokens
          ?? this.config?.options?.generationConfig?.maxOutputTokens
          ?? 4000,
        stopSequences: params.options?.stopSequences
      }
    });

    try {
      const streamResp = await modelInstance.generateContentStream(params.prompt);
      for await (const chunk of streamResp.stream) {
        const t = chunk.text();
        if (t) yield t;
      }
    } catch (error) {
      console.error('Gemini streaming error:', error);
      throw new Error(`Gemini streaming failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }
}


/// src/llm/providers/openai.ts ///


import OpenAI from "openai";
import {
  OpenAIModels
} from "../types";

export class OpenAIProvider extends BaseProvider {
  private openai: OpenAI;
  private config?: MultiAIConfig['providers'][AIProvider.OPENAI];

  constructor(apiKey: string, defaultModel: OpenAIModels, config?: MultiAIConfig['providers'][AIProvider.OPENAI]) {
    super(defaultModel);
    this.config = config;
    this.openai = new OpenAI({
      apiKey,
      baseURL: config?.options?.baseURL,
      organization: config?.options?.organization,
      project: config?.options?.project,
    });
  }

  async generateContent(params: {
    prompt: string;
    systemInstruction?: string;
    model?: AIModel;
    options?: GenerateContentParams['options'];
  }): Promise<AIResponse> {
    const model = (params.model as OpenAIModels) || (this.defaultModel as OpenAIModels);

    const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
      ...(params.systemInstruction ? [{ role: 'system', content: params.systemInstruction }] : []),
      { role: 'user', content: params.prompt }
    ];

    try {
      const completion = await this.openai.chat.completions.create({
        model,
        messages,
        temperature: params.options?.temperature ?? this.config?.options?.defaultQuery?.temperature,
        max_tokens: params.options?.maxTokens ?? this.config?.options?.defaultQuery?.max_tokens ?? 4000,
        top_p: params.options?.topP ?? this.config?.options?.defaultQuery?.top_p,
        frequency_penalty: this.config?.options?.defaultQuery?.frequency_penalty,
        presence_penalty: this.config?.options?.defaultQuery?.presence_penalty,
        stop: params.options?.stopSequences,
        stream: false,
      });

      const choice = completion.choices[0];

      return {
        text: choice?.message?.content || '',
        provider: AIProvider.OPENAI,
        model,
        usage: {
          promptTokens: completion.usage?.prompt_tokens,
          completionTokens: completion.usage?.completion_tokens,
          totalTokens: completion.usage?.total_tokens,
        },
        finishReason: choice?.finish_reason || undefined,
        cached: false,
      };
    } catch (error) {
      console.error('OpenAI API error:', error);
      throw new Error(`OpenAI generation failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  async *streamContent(params: {
    prompt: string;
    systemInstruction?: string;
    model?: AIModel;
    options?: GenerateContentParams['options'];
  }): AsyncIterable<string> {
    const model = (params.model as OpenAIModels) || (this.defaultModel as OpenAIModels);

    const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
      ...(params.systemInstruction ? [{ role: 'system', content: params.systemInstruction }] : []),
      { role: 'user', content: params.prompt }
    ];

    try {
      const stream = await this.openai.chat.completions.create({
        model,
        messages,
        temperature: params.options?.temperature,
        max_tokens: params.options?.maxTokens ?? 4000,
        top_p: params.options?.topP,
        stop: params.options?.stopSequences,
        stream: true,
      });

      for await (const chunk of stream) {
        const content = chunk.choices?.[0]?.delta?.content;
        if (content) yield content;
      }
    } catch (error) {
      console.error('OpenAI streaming error:', error);
      throw new Error(`OpenAI streaming failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }
}


/// src/llm/providers/anthropic.ts ///


import Anthropic, { type MessageStreamEvent } from "@anthropic-ai/sdk";
import {
  AnthropicModels
} from "../types";

export class AnthropicProvider extends BaseProvider {
  private anthropic: Anthropic;
  private config?: MultiAIConfig['providers'][AIProvider.ANTHROPIC];

  constructor(apiKey: string, defaultModel: AnthropicModels, config?: MultiAIConfig['providers'][AIProvider.ANTHROPIC]) {
    super(defaultModel);
    this.config = config;
    this.anthropic = new Anthropic({
      apiKey,
      baseURL: config?.options?.baseURL,
      defaultHeaders: config?.options?.defaultHeaders,
    });
  }

  async generateContent(params: {
    prompt: string;
    systemInstruction?: string;
    model?: AIModel;
    options?: GenerateContentParams['options'];
  }): Promise<AIResponse> {
    const model = (params.model as AnthropicModels) || (this.defaultModel as AnthropicModels);

    try {
      const msg = await this.anthropic.messages.create({
        model,
        max_tokens: params.options?.maxTokens ?? this.config?.options?.defaultQuery?.max_tokens ?? 4000,
        temperature: params.options?.temperature ?? this.config?.options?.defaultQuery?.temperature,
        top_p: params.options?.topP ?? this.config?.options?.defaultQuery?.top_p,
        top_k: this.config?.options?.defaultQuery?.top_k,
        system: params.systemInstruction,
        stop_sequences: params.options?.stopSequences,
        messages: [{ role: "user", content: params.prompt }],
        stream: false,
      });

      const first = msg.content?.[0];
      const text = (first && first.type === "text") ? first.text : "";

      return {
        text,
        provider: AIProvider.ANTHROPIC,
        model,
        usage: {
          promptTokens: msg.usage?.input_tokens,
          completionTokens: msg.usage?.output_tokens,
          totalTokens: (msg.usage?.input_tokens ?? 0) + (msg.usage?.output_tokens ?? 0),
        },
        finishReason: msg.stop_reason ?? undefined,
        cached: false,
      };
    } catch (error) {
      console.error("Anthropic API error:", error);
      throw new Error(`Anthropic generation failed: ${error instanceof Error ? error.message : "Unknown error"}`);
    }
  }

  async *streamContent(params: {
    prompt: string;
    systemInstruction?: string;
    model?: AIModel;
    options?: GenerateContentParams['options'];
  }): AsyncIterable<string> {
    const model = (params.model as AnthropicModels) || (this.defaultModel as AnthropicModels);

    try {
      const stream = await this.anthropic.messages.create({
        model,
        max_tokens: params.options?.maxTokens ?? 4000,
        temperature: params.options?.temperature,
        top_p: params.options?.topP,
        system: params.systemInstruction,
        stop_sequences: params.options?.stopSequences,
        messages: [{ role: "user", content: params.prompt }],
        stream: true,
      });

      for await (const ev of stream as AsyncIterable<MessageStreamEvent>) {
        if (ev.type === "content_block_delta" && ev.delta?.type === "text_delta" && ev.delta.text) {
          yield ev.delta.text;
        }
      }
    } catch (error) {
      console.error("Anthropic streaming error:", error);
      throw new Error(`Anthropic streaming failed: ${error instanceof Error ? error.message : "Unknown error"}`);
    }
  }
}


/// src/llm/wrapper/MultiAIWrapper.ts ///


import { AnthropicProvider } from "../providers/anthropic";
import { GeminiProvider } from "../providers/gemini";
import { OpenAIProvider } from "../providers/openai";
import {
  CraftPromptParams, RefactorCodeParams
} from "../types";

export class MultiAIWrapper {
  private providers: Map<AIProvider, { generateContent: any; streamContent?: any }>;
  private config: MultiAIConfig;
  private cache: Map<string, AIResponse>;

  private readonly SYSTEM_INSTRUCTIONS: Record<string, string> = {
    'Code Generation': 'You are an expert software developer. Generate clean, efficient, and well-documented code based on the requirements provided.',
    'Code Refactoring': 'You are an expert code reviewer. Refactor the provided code to improve readability, performance, and maintainability while preserving functionality.',
    'General Summarization': 'You are a helpful assistant that creates clear and concise summaries.',
    'Documentation': 'You are a technical writer. Create comprehensive and clear documentation.',
    'Bug Fixing': 'You are a debugging expert. Analyze the code and fix any bugs or issues found.',
    'Code Review': 'You are an experienced code reviewer. Provide constructive feedback on code quality, best practices, and potential improvements.',
    'API Design': 'You are an API design expert. Create well-structured, RESTful APIs with proper documentation.',
    'Database Design': 'You are a database architect. Design efficient, normalized database schemas.',
    'Testing': 'You are a testing expert. Write comprehensive unit tests and integration tests.',
    'Security Audit': 'You are a security expert. Identify potential security vulnerabilities and recommend fixes.',
  };

  constructor(config: MultiAIConfig) {
    this.config = config;
    this.providers = new Map();
    this.cache = new Map();
    this.initializeProviders();
  }

  private initializeProviders(): void {
    const { providers } = this.config;

    if (providers[AIProvider.GEMINI]) {
      const gem = providers[AIProvider.GEMINI]!;
      this.providers.set(
        AIProvider.GEMINI,
        new GeminiProvider(gem.apiKey, gem.defaultModel, gem)
      );
    }

    if (providers[AIProvider.OPENAI]) {
      const oai = providers[AIProvider.OPENAI]!;
      this.providers.set(
        AIProvider.OPENAI,
        new OpenAIProvider(oai.apiKey, oai.defaultModel, oai)
      );
    }

    if (providers[AIProvider.ANTHROPIC]) {
      const claude = providers[AIProvider.ANTHROPIC]!;
      this.providers.set(
        AIProvider.ANTHROPIC,
        new AnthropicProvider(claude.apiKey, claude.defaultModel, claude)
      );
    }
  }

  private generateCacheKey(params: any): string {
    return JSON.stringify(params);
  }

  private getProvider(providerType?: AIProvider) {
    const target = providerType || this.config.defaultProvider;
    const p = this.providers.get(target);
    if (!p) throw new Error(`Provider ${target} not configured or initialized`);
    return { instance: p, type: target };
  }

  public async generateContent(params: GenerateContentParams): Promise<AIResponse> {
    const { instance, type } = this.getProvider(params.provider);
    const cacheKey = this.generateCacheKey({ ...params, provider: type });

    if (this.config.enableCache && this.cache.has(cacheKey)) {
      return { ...this.cache.get(cacheKey)!, cached: true };
    }

    const response = await instance.generateContent({
      prompt: params.prompt,
      systemInstruction: params.systemInstruction,
      model: params.model as AIModel,
      options: params.options,
    });

    if (this.config.enableCache) this.cache.set(cacheKey, response);
    return response;
  }

  public async *streamContent(params: GenerateContentParams): AsyncIterable<string> {
    const { instance, type } = this.getProvider(params.provider);
    if (!instance.streamContent) throw new Error(`Provider ${type} does not support streaming`);

    yield* instance.streamContent({
      prompt: params.prompt,
      systemInstruction: params.systemInstruction,
      model: params.model as AIModel,
      options: params.options,
    });
  }

  public async craftPrompt(params: CraftPromptParams): Promise<AIResponse> {
    const systemInstruction =
      this.SYSTEM_INSTRUCTIONS[params.purpose] ?? this.SYSTEM_INSTRUCTIONS['General Summarization'];

    const userContent = `**Purpose:** ${params.purpose}
**Raw Ideas & Requirements:**
${params.ideas.map(i => `- ${i}`).join('\n')}`;

    return this.generateContent({
      prompt: userContent,
      systemInstruction,
      provider: params.provider,
      model: params.model,
      options: params.options,
    });
  }

  public async refactorCode(params: RefactorCodeParams): Promise<AIResponse> {
    return this.generateContent({
      prompt: params.code,
      systemInstruction: params.systemPrompt,
      provider: params.provider,
      model: params.model,
      options: params.options,
    });
  }

  public getAvailableProviders(): AIProvider[] {
    return Array.from(this.providers.keys());
  }

  public clearCache(): void {
    this.cache.clear();
  }

  public getCacheStats(): { size: number; keys: string[] } {
    return { size: this.cache.size, keys: Array.from(this.cache.keys()) };
  }

  public async testProvider(provider: AIProvider): Promise<boolean> {
    try {
      const response = await this.generateContent({
        prompt: "Return exactly: Test successful.",
        provider,
        options: { maxTokens: 30, temperature: 0, stopSequences: ["\n"] }
      });
      return /Test successful/.test(response.text);
    } catch {
      return false;
    }
  }

  public getUsageInfo(response: AIResponse): string {
    if (!response.usage) return 'Usage information not available';
    const { promptTokens, completionTokens, totalTokens } = response.usage;
    return `Tokens - Prompt: ${promptTokens ?? 'N/A'}, Completion: ${completionTokens ?? 'N/A'}, Total: ${totalTokens ?? 'N/A'}`;
  }
}


/// src/llm/index.ts ///


export * from "./providers/anthropic";
export * from "./providers/gemini";
export * from "./providers/openai";
export * from "./types";
export * from "./wrapper/MultiAIWrapper";

